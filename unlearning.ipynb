{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n# Note: sympy<1.13 is required for torch-cka compatibility\n!pip install -q timm umap-learn \"sympy<1.13\" torch-cka huggingface_hub\n\nprint(\"Packages installed.\")\nprint(\"If this is your first run, go to Runtime -> Restart runtime, then run all cells again.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup path to find utils.py (uploaded to /content/)\nimport sys\nif '/content' not in sys.path:\n    sys.path.append('/content')\n\n# Verify the file exists\nimport os\nif not os.path.exists('/content/utils.py'):\n    print(\"WARNING: Please upload utils.py using the Files tab\")\nelse:\n    print(\"Setup complete - utils.py found\")\n\nimport time\nimport copy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom typing import Dict\n\nfrom utils import (\n    get_resnet18, get_vgg16bn, get_data_loaders, get_umap_subset,\n    create_results_json, save_results, SEED\n)\n\n# Set random seeds\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "\n",
    "FORGET_CLASS = 0        # Class to unlearn (0-9 for CIFAR-10)\n",
    "EPOCHS = 3              # Number of unlearning epochs\n",
    "BATCH_SIZE = 32         # Batch size\n",
    "LEARNING_RATE = 0.0001  # Learning rate\n",
    "MOMENTUM = 0.9          # SGD momentum\n",
    "WEIGHT_DECAY = 5e-4     # Weight decay\n",
    "NUM_CLASSES = 10        # CIFAR-10 classes\n",
    "\n",
    "# Method-specific parameters\n",
    "MAX_GRAD_NORM = 100.0      # Gradient clipping for Gradient Ascent\n",
    "SALIENCY_THRESHOLD = 0.75  # Top 75% weights for SalUn\n",
    "\n",
    "# Device selection\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Forget class: {FORGET_CLASS}\")\n",
    "print(f\"Epochs: {EPOCHS}, Batch size: {BATCH_SIZE}, LR: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlearning Method Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Method 1: Random Labeling\n",
    "# ============================================================================\n",
    "\n",
    "def random_labeling_unlearn(\n",
    "    model: nn.Module,\n",
    "    retain_loader: DataLoader,\n",
    "    forget_loader: DataLoader,\n",
    "    forget_class: int,\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    "    device: torch.device,\n",
    "    momentum: float = 0.9,\n",
    "    weight_decay: float = 5e-4\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Random Labeling Unlearning Method.\n",
    "    Combines retain and forget data. For forget class samples,\n",
    "    assigns random labels from remaining classes.\n",
    "    \"\"\"\n",
    "    combined_dataset = ConcatDataset([retain_loader.dataset, forget_loader.dataset])\n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    remain_classes = [i for i in range(NUM_CLASSES) if i != forget_class]\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in combined_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Assign random labels to forget class\n",
    "            forget_mask = (labels == forget_class)\n",
    "            if forget_mask.sum() > 0:\n",
    "                random_labels = torch.tensor([\n",
    "                    remain_classes[torch.randint(0, len(remain_classes), (1,)).item()]\n",
    "                    for _ in range(forget_mask.sum())\n",
    "                ], device=device)\n",
    "                labels[forget_mask] = random_labels\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"  Epoch [{epoch+1}/{epochs}] Loss: {running_loss/len(combined_loader):.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Method 2: Gradient Ascent\n",
    "# ============================================================================\n",
    "\n",
    "def gradient_ascent_unlearn(\n",
    "    model: nn.Module,\n",
    "    forget_loader: DataLoader,\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    "    device: torch.device,\n",
    "    max_grad_norm: float = 100.0,\n",
    "    momentum: float = 0.9,\n",
    "    weight_decay: float = 5e-4\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Gradient Ascent Unlearning Method.\n",
    "    Trains ONLY on forget data using NEGATIVE cross-entropy loss.\n",
    "    \"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in forget_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = -criterion(outputs, labels)  # NEGATIVE loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            running_loss += (-loss.item())\n",
    "        \n",
    "        print(f\"  Epoch [{epoch+1}/{epochs}] Loss: {running_loss/len(forget_loader):.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Method 3: SalUn (Saliency-based Unlearning)\n",
    "# ============================================================================\n",
    "\n",
    "def compute_gradient_saliency(\n",
    "    model: nn.Module,\n",
    "    forget_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    threshold: float = 0.75,\n",
    "    max_batches: int = 5\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Compute gradient-based weight saliency mask.\"\"\"\n",
    "    print(\"  Computing gradient-based weight saliency...\")\n",
    "    \n",
    "    gradient_dict = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            gradient_dict[name] = torch.zeros_like(param)\n",
    "    \n",
    "    model.eval()\n",
    "    batch_count = 0\n",
    "    \n",
    "    for inputs, labels in forget_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = -criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                gradient_dict[name] += param.grad.abs()\n",
    "        \n",
    "        batch_count += 1\n",
    "        if batch_count >= max_batches:\n",
    "            break\n",
    "    \n",
    "    for name in gradient_dict:\n",
    "        gradient_dict[name] /= batch_count\n",
    "    \n",
    "    all_grads = torch.cat([gradient_dict[name].flatten() for name in gradient_dict])\n",
    "    k = int(threshold * len(all_grads))\n",
    "    threshold_value = torch.topk(all_grads, k)[0][-1] if k > 0 else float('inf')\n",
    "    \n",
    "    mask = {name: (gradient_dict[name] >= threshold_value).float().to(device) for name in gradient_dict}\n",
    "    \n",
    "    total_params = sum(m.numel() for m in mask.values())\n",
    "    selected_params = sum(m.sum().item() for m in mask.values())\n",
    "    print(f\"  Saliency mask: {int(selected_params):,}/{total_params:,} params ({selected_params/total_params*100:.1f}%)\")\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def salun_unlearn(\n",
    "    model: nn.Module,\n",
    "    retain_loader: DataLoader,\n",
    "    forget_loader: DataLoader,\n",
    "    forget_class: int,\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    "    device: torch.device,\n",
    "    saliency_threshold: float = 0.75,\n",
    "    grad_clip: float = 100.0,\n",
    "    momentum: float = 0.9,\n",
    "    weight_decay: float = 5e-4\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    SalUn (Saliency-based Unlearning) Method.\n",
    "    Two-phase training with saliency-masked gradient updates.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    remain_classes = [i for i in range(NUM_CLASSES) if i != forget_class]\n",
    "    \n",
    "    saliency_mask = compute_gradient_saliency(model, forget_loader, criterion, device, saliency_threshold)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    \n",
    "    def apply_saliency_mask():\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in saliency_mask and param.grad is not None:\n",
    "                    param.grad *= saliency_mask[name]\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        total_batches = 0\n",
    "        \n",
    "        # Phase 1: Forget data with random labels\n",
    "        for inputs, labels in forget_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            random_labels = torch.tensor([\n",
    "                remain_classes[torch.randint(0, len(remain_classes), (1,)).item()]\n",
    "                for _ in range(len(labels))\n",
    "            ], device=device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, random_labels)\n",
    "            loss.backward()\n",
    "            apply_saliency_mask()\n",
    "            if grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            total_batches += 1\n",
    "        \n",
    "        # Phase 2: Retain data normally\n",
    "        for inputs, labels in retain_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            apply_saliency_mask()\n",
    "            if grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            total_batches += 1\n",
    "        \n",
    "        print(f\"  Epoch [{epoch+1}/{epochs}] Avg Loss: {running_loss/total_batches:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data once for all experiments\n",
    "print(\"Loading CIFAR-10 data...\")\n",
    "train_loader, test_loader, retain_loader, forget_loader, train_set, test_set = \\\n",
    "    get_data_loaders(BATCH_SIZE, FORGET_CLASS)\n",
    "\n",
    "print(\"Preparing UMAP subset...\")\n",
    "umap_subset, umap_loader, selected_indices = get_umap_subset(train_set, test_set)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_loader.dataset):,}\")\n",
    "print(f\"  Test: {len(test_loader.dataset):,}\")\n",
    "print(f\"  Retain: {len(retain_loader.dataset):,}\")\n",
    "print(f\"  Forget: {len(forget_loader.dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Methods on All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and methods\n",
    "models_config = [\n",
    "    (\"ResNet-18\", get_resnet18),\n",
    "    (\"VGG-16-BN\", get_vgg16bn)\n",
    "]\n",
    "\n",
    "methods_config = [\n",
    "    (\"RandomLabeling\", lambda m, rl, fl, fc, e, lr, d: random_labeling_unlearn(\n",
    "        m, rl, fl, fc, e, lr, d, MOMENTUM, WEIGHT_DECAY)),\n",
    "    (\"GradientAscent\", lambda m, rl, fl, fc, e, lr, d: gradient_ascent_unlearn(\n",
    "        m, fl, e, lr, d, MAX_GRAD_NORM, MOMENTUM, WEIGHT_DECAY)),\n",
    "    (\"SalUn\", lambda m, rl, fl, fc, e, lr, d: salun_unlearn(\n",
    "        m, rl, fl, fc, e, lr, d, SALIENCY_THRESHOLD, MAX_GRAD_NORM, MOMENTUM, WEIGHT_DECAY))\n",
    "]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for model_name, model_fn in models_config:\n",
    "    for method_name, method_fn in methods_config:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Running {method_name} on {model_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Load fresh pretrained model\n",
    "        print(f\"Loading pretrained {model_name}...\")\n",
    "        model = model_fn().to(device)\n",
    "        original_model = copy.deepcopy(model)\n",
    "        \n",
    "        # Run unlearning\n",
    "        print(f\"Starting {method_name} unlearning...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model = method_fn(model, retain_loader, forget_loader, FORGET_CLASS, EPOCHS, LEARNING_RATE, device)\n",
    "        \n",
    "        runtime = time.time() - start_time\n",
    "        print(f\"\\nUnlearning completed in {runtime:.2f} seconds\")\n",
    "        \n",
    "        # Generate results\n",
    "        print(f\"Generating results...\")\n",
    "        result = create_results_json(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            umap_subset=umap_subset,\n",
    "            umap_loader=umap_loader,\n",
    "            selected_indices=selected_indices,\n",
    "            forget_class=FORGET_CLASS,\n",
    "            method_name=method_name,\n",
    "            model_name=model_name,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            runtime=runtime,\n",
    "            device=device,\n",
    "            original_model=original_model\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        save_results(result, model, model_name, FORGET_CLASS, output_dir=\"backend/data\")\n",
    "        all_results.append((model_name, result))\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n{'-'*40}\")\n",
    "        print(f\"Results for {model_name} + {method_name}:\")\n",
    "        print(f\"  UA: {result['UA']:.3f}  RA: {result['RA']:.3f}\")\n",
    "        print(f\"  TUA: {result['TUA']:.3f}  TRA: {result['TRA']:.3f}\")\n",
    "        print(f\"  FQS: {result['FQS']}  PA: {result['PA']}  Runtime: {result['RTE']:.1f}s\")\n",
    "        print(f\"{'-'*40}\")\n",
    "        \n",
    "        # Clean up to free memory\n",
    "        del model, original_model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive comparison table\n",
    "print(\"\\n\" + \"=\"*95)\n",
    "print(\"COMPARISON: All Unlearning Methods\")\n",
    "print(\"=\"*95)\n",
    "print(f\"{'Model':<12} {'Method':<16} {'UA':>7} {'RA':>7} {'TUA':>7} {'TRA':>7} {'FQS':>7} {'PA':>7} {'Time':>8}\")\n",
    "print(\"-\"*95)\n",
    "\n",
    "for model_name, r in all_results:\n",
    "    print(f\"{model_name:<12} {r['Method']:<16} {r['UA']:>7.3f} {r['RA']:>7.3f} {r['TUA']:>7.3f} {r['TRA']:>7.3f} {r['FQS']:>7.4f} {r['PA']:>7.4f} {r['RTE']:>7.1f}s\")\n",
    "\n",
    "print(\"=\"*95)\n",
    "print(\"\\nMetric Definitions:\")\n",
    "print(\"  UA  = Unlearning Accuracy (accuracy on forget class in training set)\")\n",
    "print(\"  RA  = Remain Accuracy (accuracy on other classes in training set)\")\n",
    "print(\"  TUA = Test Unlearning Accuracy (accuracy on forget class in test set)\")\n",
    "print(\"  TRA = Test Remain Accuracy (accuracy on other classes in test set)\")\n",
    "print(\"  FQS = Forgetting Quality Score (higher = better forgetting)\")\n",
    "print(\"  PA  = Privacy Attack score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary by method (average across models)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY BY METHOD (averaged across models)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "methods = [\"RandomLabeling\", \"GradientAscent\", \"SalUn\"]\n",
    "for method in methods:\n",
    "    method_results = [(m, r) for m, r in all_results if r['Method'] == method]\n",
    "    if method_results:\n",
    "        avg_ua = sum(r['UA'] for _, r in method_results) / len(method_results)\n",
    "        avg_ra = sum(r['RA'] for _, r in method_results) / len(method_results)\n",
    "        avg_tua = sum(r['TUA'] for _, r in method_results) / len(method_results)\n",
    "        avg_tra = sum(r['TRA'] for _, r in method_results) / len(method_results)\n",
    "        avg_fqs = sum(r['FQS'] for _, r in method_results) / len(method_results)\n",
    "        avg_pa = sum(r['PA'] for _, r in method_results) / len(method_results)\n",
    "        avg_rte = sum(r['RTE'] for _, r in method_results) / len(method_results)\n",
    "        print(f\"{method:<16}: UA={avg_ua:.3f} RA={avg_ra:.3f} TUA={avg_tua:.3f} TRA={avg_tra:.3f} FQS={avg_fqs:.4f} PA={avg_pa:.4f} Time={avg_rte:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY BY MODEL (averaged across methods)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = [\"ResNet-18\", \"VGG-16-BN\"]\n",
    "for model in models:\n",
    "    model_results = [(m, r) for m, r in all_results if m == model]\n",
    "    if model_results:\n",
    "        avg_ua = sum(r['UA'] for _, r in model_results) / len(model_results)\n",
    "        avg_ra = sum(r['RA'] for _, r in model_results) / len(model_results)\n",
    "        avg_tua = sum(r['TUA'] for _, r in model_results) / len(model_results)\n",
    "        avg_tra = sum(r['TRA'] for _, r in model_results) / len(model_results)\n",
    "        avg_fqs = sum(r['FQS'] for _, r in model_results) / len(model_results)\n",
    "        avg_pa = sum(r['PA'] for _, r in model_results) / len(model_results)\n",
    "        avg_rte = sum(r['RTE'] for _, r in model_results) / len(model_results)\n",
    "        print(f\"{model:<12}: UA={avg_ua:.3f} RA={avg_ra:.3f} TUA={avg_tua:.3f} TRA={avg_tra:.3f} FQS={avg_fqs:.4f} PA={avg_pa:.4f} Time={avg_rte:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Expected Behavior:\n",
    "- **Lower UA/TUA** = Better forgetting (model no longer recognizes forget class)\n",
    "- **Higher RA/TRA** = Better retention (model still performs well on other classes)\n",
    "- **Higher FQS** = Better overall forgetting quality\n",
    "\n",
    "### Method Characteristics:\n",
    "1. **Random Labeling**: Gentle approach, may not fully forget but preserves retention well\n",
    "2. **Gradient Ascent**: Aggressive forgetting, may harm retention (catastrophic forgetting risk)\n",
    "3. **SalUn**: Balanced approach, targets only relevant weights\n",
    "\n",
    "### Architectural Differences (Research Question):\n",
    "- **ResNet-18**: Skip connections may make forgetting more difficult (information preserved across layers)\n",
    "- **VGG-16**: Sequential architecture may allow more localized forgetting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}