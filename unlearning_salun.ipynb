{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SalUn (Saliency-based Unlearning)\n",
    "\n",
    "This notebook implements the SalUn unlearning method on ResNet-18 and VGG-16 models.\n",
    "\n",
    "**Method**: SalUn combines gradient saliency with random labeling:\n",
    "1. **Compute Saliency Mask**: Identify the top-k% most important weights for the forget class\n",
    "2. **Two-Phase Training**: \n",
    "   - Phase 1: Process forget data with random labels (saliency-masked gradients)\n",
    "   - Phase 2: Process retain data normally (saliency-masked gradients)\n",
    "\n",
    "**Key Insight**: Only update the weights that are most relevant to the forget class.\n",
    "\n",
    "**Reference**: Based on the implementation in `app/threads/unlearn_SalUn_thread.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q timm umap-learn\n",
    "\n",
    "# Setup path to find unlearning_utils.py (uploaded to /content/)\n",
    "import sys\n",
    "if '/content' not in sys.path:\n",
    "    sys.path.append('/content')\n",
    "\n",
    "# Verify the file exists\n",
    "import os\n",
    "if not os.path.exists('/content/unlearning_utils.py'):\n",
    "    print(\"⚠️  ERROR: Please upload unlearning_utils.py using the Files tab\")\n",
    "else:\n",
    "    print(\"✓ Setup complete - unlearning_utils.py found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Dict\n",
    "\n",
    "from unlearning_utils import (\n",
    "    get_resnet18, get_vgg16bn, get_data_loaders, get_umap_subset,\n",
    "    create_results_json, save_results, SEED\n",
    ")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "FORGET_CLASS = 0        # Class to unlearn (0-9 for CIFAR-10)\n",
    "EPOCHS = 5              # Number of unlearning epochs\n",
    "BATCH_SIZE = 128        # Batch size\n",
    "LEARNING_RATE = 0.1     # Learning rate\n",
    "MOMENTUM = 0.9          # SGD momentum\n",
    "WEIGHT_DECAY = 5e-4     # Weight decay\n",
    "SALIENCY_THRESHOLD = 0.75  # Select top 75% most salient weights\n",
    "GRAD_CLIP = 100.0       # Gradient clipping threshold\n",
    "NUM_CLASSES = 10        # CIFAR-10 classes\n",
    "\n",
    "# Device selection\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Forget class: {FORGET_CLASS}\")\n",
    "print(f\"Epochs: {EPOCHS}, Batch size: {BATCH_SIZE}, LR: {LEARNING_RATE}\")\n",
    "print(f\"Saliency threshold: {SALIENCY_THRESHOLD} (top {SALIENCY_THRESHOLD*100:.0f}% weights)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_saliency(\n",
    "    model: nn.Module,\n",
    "    forget_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    threshold: float = 0.75,\n",
    "    max_batches: int = 5\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute gradient-based weight saliency mask.\n",
    "    \n",
    "    Identifies which weights are most important for the forget class by\n",
    "    computing gradient magnitudes on forget data.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to analyze\n",
    "        forget_loader: DataLoader for forget samples\n",
    "        criterion: Loss function\n",
    "        device: Device to compute on\n",
    "        threshold: Fraction of top weights to select (e.g., 0.75 = top 75%)\n",
    "        max_batches: Maximum batches to use for gradient computation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping parameter names to binary masks\n",
    "    \"\"\"\n",
    "    print(\"Computing gradient-based weight saliency...\")\n",
    "    \n",
    "    # Initialize gradient accumulator\n",
    "    gradient_dict = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            gradient_dict[name] = torch.zeros_like(param)\n",
    "    \n",
    "    model.eval()\n",
    "    batch_count = 0\n",
    "    \n",
    "    # Compute gradients on forget dataset\n",
    "    for inputs, labels in forget_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Use negative loss for gradient ascent direction\n",
    "        loss = -criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Accumulate gradient magnitudes\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                gradient_dict[name] += param.grad.abs()\n",
    "        \n",
    "        batch_count += 1\n",
    "        if batch_count >= max_batches:\n",
    "            break\n",
    "    \n",
    "    # Normalize by number of batches\n",
    "    for name in gradient_dict:\n",
    "        gradient_dict[name] /= batch_count\n",
    "    \n",
    "    # Compute threshold for top-k% weights\n",
    "    all_grads = torch.cat([gradient_dict[name].flatten() for name in gradient_dict])\n",
    "    k = int(threshold * len(all_grads))\n",
    "    \n",
    "    if k > 0:\n",
    "        topk_values, _ = torch.topk(all_grads, k)\n",
    "        threshold_value = topk_values[-1]\n",
    "    else:\n",
    "        threshold_value = float('inf')\n",
    "    \n",
    "    # Create binary mask\n",
    "    mask = {}\n",
    "    for name in gradient_dict:\n",
    "        mask[name] = (gradient_dict[name] >= threshold_value).float().to(device)\n",
    "    \n",
    "    # Count selected parameters\n",
    "    total_params = sum(m.numel() for m in mask.values())\n",
    "    selected_params = sum(m.sum().item() for m in mask.values())\n",
    "    print(f\"Saliency mask: {int(selected_params):,}/{total_params:,} parameters selected ({selected_params/total_params*100:.1f}%)\")\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salun_unlearn(\n",
    "    model: nn.Module,\n",
    "    retain_loader: DataLoader,\n",
    "    forget_loader: DataLoader,\n",
    "    forget_class: int,\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    "    device: torch.device,\n",
    "    saliency_threshold: float = 0.75,\n",
    "    grad_clip: float = 100.0,\n",
    "    momentum: float = 0.9,\n",
    "    weight_decay: float = 5e-4\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    SalUn (Saliency-based Unlearning) Method.\n",
    "    \n",
    "    Two-phase training with saliency-masked gradient updates:\n",
    "    1. Compute saliency mask on forget data\n",
    "    2. For each epoch:\n",
    "       - Phase 1: Process forget data with random labels\n",
    "       - Phase 2: Process retain data normally\n",
    "       - Both phases use saliency-masked gradients\n",
    "    \n",
    "    Args:\n",
    "        model: Model to unlearn\n",
    "        retain_loader: DataLoader for retain samples\n",
    "        forget_loader: DataLoader for forget samples\n",
    "        forget_class: Class index to forget\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        device: Device to train on\n",
    "        saliency_threshold: Fraction of top weights to update\n",
    "        grad_clip: Maximum gradient norm\n",
    "        momentum: SGD momentum\n",
    "        weight_decay: L2 regularization\n",
    "    \n",
    "    Returns:\n",
    "        Unlearned model\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    remain_classes = [i for i in range(NUM_CLASSES) if i != forget_class]\n",
    "    \n",
    "    # Step 1: Compute saliency mask\n",
    "    saliency_mask = compute_gradient_saliency(\n",
    "        model, forget_loader, criterion, device, saliency_threshold\n",
    "    )\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    def apply_saliency_mask():\n",
    "        \"\"\"Apply saliency mask to gradients.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in saliency_mask and param.grad is not None:\n",
    "                    param.grad *= saliency_mask[name]\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        total_batches = 0\n",
    "        \n",
    "        print(f\"\\nEpoch [{epoch+1}/{epochs}]\")\n",
    "        \n",
    "        # Phase 1: Process forget data with random labels\n",
    "        print(\"  Phase 1: Forget data with random labels...\")\n",
    "        for inputs, labels in forget_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Assign random labels from remaining classes\n",
    "            random_labels = torch.tensor([\n",
    "                remain_classes[torch.randint(0, len(remain_classes), (1,)).item()]\n",
    "                for _ in range(len(labels))\n",
    "            ], device=device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, random_labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply saliency mask to gradients\n",
    "            apply_saliency_mask()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            total_batches += 1\n",
    "        \n",
    "        # Phase 2: Process retain data normally\n",
    "        print(\"  Phase 2: Retain data with normal labels...\")\n",
    "        for inputs, labels in retain_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply saliency mask to gradients\n",
    "            apply_saliency_mask()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            total_batches += 1\n",
    "        \n",
    "        avg_loss = running_loss / total_batches\n",
    "        print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading CIFAR-10 data...\")\n",
    "train_loader, test_loader, retain_loader, forget_loader, train_set, test_set = \\\n",
    "    get_data_loaders(BATCH_SIZE, FORGET_CLASS)\n",
    "\n",
    "# Prepare UMAP subset\n",
    "print(\"Preparing UMAP subset...\")\n",
    "umap_subset, umap_loader, selected_indices = get_umap_subset(train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to evaluate\n",
    "models = [\n",
    "    (\"ResNet-18\", get_resnet18),\n",
    "    (\"VGG-16-BN\", get_vgg16bn)\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model_fn in models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running SalUn on {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load fresh pretrained model\n",
    "    print(f\"Loading pretrained {model_name}...\")\n",
    "    model = model_fn().to(device)\n",
    "    \n",
    "    # Keep a copy of original model for CKA comparison (optional)\n",
    "    original_model = copy.deepcopy(model)\n",
    "    \n",
    "    # Run unlearning\n",
    "    print(f\"\\nStarting SalUn unlearning...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = salun_unlearn(\n",
    "        model=model,\n",
    "        retain_loader=retain_loader,\n",
    "        forget_loader=forget_loader,\n",
    "        forget_class=FORGET_CLASS,\n",
    "        epochs=EPOCHS,\n",
    "        lr=LEARNING_RATE,\n",
    "        device=device,\n",
    "        saliency_threshold=SALIENCY_THRESHOLD,\n",
    "        grad_clip=GRAD_CLIP,\n",
    "        momentum=MOMENTUM,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    runtime = time.time() - start_time\n",
    "    print(f\"\\nUnlearning completed in {runtime:.2f} seconds\")\n",
    "    \n",
    "    # Generate results\n",
    "    print(f\"\\nGenerating results...\")\n",
    "    result = create_results_json(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        umap_subset=umap_subset,\n",
    "        umap_loader=umap_loader,\n",
    "        selected_indices=selected_indices,\n",
    "        forget_class=FORGET_CLASS,\n",
    "        method_name=\"SalUn\",\n",
    "        model_name=model_name,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        runtime=runtime,\n",
    "        device=device,\n",
    "        original_model=original_model\n",
    "    )\n",
    "    \n",
    "    # Add SalUn-specific info\n",
    "    result['saliency_threshold'] = SALIENCY_THRESHOLD\n",
    "    \n",
    "    # Save results\n",
    "    save_results(result, model, output_dir=\"notebook_results\")\n",
    "    results.append(result)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'-'*40}\")\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    print(f\"  UA (Unlearning Accuracy):  {result['UA']:.3f}\")\n",
    "    print(f\"  RA (Remain Accuracy):      {result['RA']:.3f}\")\n",
    "    print(f\"  TUA (Test Unlearn Acc):    {result['TUA']:.3f}\")\n",
    "    print(f\"  TRA (Test Remain Acc):     {result['TRA']:.3f}\")\n",
    "    print(f\"  FQS (Forgetting Quality):  {result['FQS']}\")\n",
    "    print(f\"  Runtime: {result['RTE']:.1f}s\")\n",
    "    print(f\"{'-'*40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: SalUn Unlearning Results\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<15} {'UA':>8} {'RA':>8} {'TUA':>8} {'TRA':>8} {'FQS':>8} {'Time':>8}\")\n",
    "print(\"-\"*70)\n",
    "for r in results:\n",
    "    print(f\"{r['Model']:<15} {r['UA']:>8.3f} {r['RA']:>8.3f} {r['TUA']:>8.3f} {r['TRA']:>8.3f} {r['FQS']:>8.4f} {r['RTE']:>7.1f}s\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on SalUn\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Uses gradient saliency to identify important weights for the forget class\n",
    "- Only updates the most salient weights (default: top 75%)\n",
    "- Combines random labeling with saliency-based weight selection\n",
    "- Two-phase training: forget data (random labels) then retain data (normal)\n",
    "\n",
    "**Advantages over other methods:**\n",
    "- More selective than gradient ascent (preserves non-salient weights)\n",
    "- More targeted than random labeling (focuses on relevant weights)\n",
    "- Better balance between forgetting and retaining\n",
    "\n",
    "**Hyperparameter Sensitivity:**\n",
    "- `saliency_threshold`: Higher = more weights updated = stronger forgetting\n",
    "- Lower threshold = more selective = better preservation of other classes\n",
    "- Typical values: 0.5-0.9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
