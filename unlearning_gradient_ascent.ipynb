{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Ascent Unlearning\n",
    "\n",
    "This notebook implements the Gradient Ascent unlearning method on ResNet-18 and VGG-16 models.\n",
    "\n",
    "**Method**: Trains ONLY on forget data using **negative cross-entropy loss** (gradient ascent).\n",
    "This increases the loss on the forget class, effectively \"unlearning\" those samples.\n",
    "\n",
    "**Key**: `loss = -criterion(outputs, labels)` makes the model maximize loss on forget samples.\n",
    "\n",
    "**Reference**: Based on the implementation in `app/threads/unlearn_GA_thread.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q timm umap-learn\n",
    "\n",
    "# Setup path to find unlearning_utils.py (uploaded to /content/)\n",
    "import sys\n",
    "if '/content' not in sys.path:\n",
    "    sys.path.append('/content')\n",
    "\n",
    "# Verify the file exists\n",
    "import os\n",
    "if not os.path.exists('/content/unlearning_utils.py'):\n",
    "    print(\"⚠️  ERROR: Please upload unlearning_utils.py using the Files tab\")\n",
    "else:\n",
    "    print(\"✓ Setup complete - unlearning_utils.py found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from unlearning_utils import (\n",
    "    get_resnet18, get_vgg16bn, get_data_loaders, get_umap_subset,\n",
    "    create_results_json, save_results, SEED\n",
    ")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "FORGET_CLASS = 0        # Class to unlearn (0-9 for CIFAR-10)\n",
    "EPOCHS = 5              # Number of unlearning epochs\n",
    "BATCH_SIZE = 128        # Batch size\n",
    "LEARNING_RATE = 0.1     # Learning rate\n",
    "MOMENTUM = 0.9          # SGD momentum\n",
    "WEIGHT_DECAY = 5e-4     # Weight decay\n",
    "MAX_GRAD_NORM = 100.0   # Gradient clipping threshold\n",
    "NUM_CLASSES = 10        # CIFAR-10 classes\n",
    "\n",
    "# Device selection\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Forget class: {FORGET_CLASS}\")\n",
    "print(f\"Epochs: {EPOCHS}, Batch size: {BATCH_SIZE}, LR: {LEARNING_RATE}\")\n",
    "print(f\"Max gradient norm: {MAX_GRAD_NORM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascent_unlearn(\n",
    "    model: nn.Module,\n",
    "    forget_loader: DataLoader,\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    "    device: torch.device,\n",
    "    max_grad_norm: float = 100.0,\n",
    "    momentum: float = 0.9,\n",
    "    weight_decay: float = 5e-4\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Gradient Ascent Unlearning Method.\n",
    "    \n",
    "    Trains ONLY on forget data using NEGATIVE cross-entropy loss.\n",
    "    This maximizes the loss on forget samples, causing the model to\n",
    "    \"forget\" the learned patterns for that class.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to unlearn\n",
    "        forget_loader: DataLoader for forget samples (class to unlearn)\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        device: Device to train on\n",
    "        max_grad_norm: Maximum gradient norm for clipping\n",
    "        momentum: SGD momentum\n",
    "        weight_decay: L2 regularization\n",
    "    \n",
    "    Returns:\n",
    "        Unlearned model\n",
    "    \"\"\"\n",
    "    # Setup optimizer and loss\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(forget_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # NEGATIVE loss for gradient ascent\n",
    "            # This maximizes the loss on forget samples\n",
    "            loss = -criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics (use positive loss for display)\n",
    "            running_loss += (-loss.item())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(forget_loader)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading CIFAR-10 data...\")\n",
    "train_loader, test_loader, retain_loader, forget_loader, train_set, test_set = \\\n",
    "    get_data_loaders(BATCH_SIZE, FORGET_CLASS)\n",
    "\n",
    "# Prepare UMAP subset\n",
    "print(\"Preparing UMAP subset...\")\n",
    "umap_subset, umap_loader, selected_indices = get_umap_subset(train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to evaluate\n",
    "models = [\n",
    "    (\"ResNet-18\", get_resnet18),\n",
    "    (\"VGG-16-BN\", get_vgg16bn)\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model_fn in models:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running Gradient Ascent on {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load fresh pretrained model\n",
    "    print(f\"Loading pretrained {model_name}...\")\n",
    "    model = model_fn().to(device)\n",
    "    \n",
    "    # Keep a copy of original model for CKA comparison (optional)\n",
    "    original_model = copy.deepcopy(model)\n",
    "    \n",
    "    # Run unlearning\n",
    "    print(f\"\\nStarting Gradient Ascent unlearning...\")\n",
    "    print(f\"Training ONLY on forget class ({FORGET_CLASS}) with NEGATIVE loss\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = gradient_ascent_unlearn(\n",
    "        model=model,\n",
    "        forget_loader=forget_loader,\n",
    "        epochs=EPOCHS,\n",
    "        lr=LEARNING_RATE,\n",
    "        device=device,\n",
    "        max_grad_norm=MAX_GRAD_NORM,\n",
    "        momentum=MOMENTUM,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    runtime = time.time() - start_time\n",
    "    print(f\"\\nUnlearning completed in {runtime:.2f} seconds\")\n",
    "    \n",
    "    # Generate results\n",
    "    print(f\"\\nGenerating results...\")\n",
    "    result = create_results_json(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        umap_subset=umap_subset,\n",
    "        umap_loader=umap_loader,\n",
    "        selected_indices=selected_indices,\n",
    "        forget_class=FORGET_CLASS,\n",
    "        method_name=\"GradientAscent\",\n",
    "        model_name=model_name,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        runtime=runtime,\n",
    "        device=device,\n",
    "        original_model=original_model\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    save_results(result, model, output_dir=\"notebook_results\")\n",
    "    results.append(result)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'-'*40}\")\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    print(f\"  UA (Unlearning Accuracy):  {result['UA']:.3f}\")\n",
    "    print(f\"  RA (Remain Accuracy):      {result['RA']:.3f}\")\n",
    "    print(f\"  TUA (Test Unlearn Acc):    {result['TUA']:.3f}\")\n",
    "    print(f\"  TRA (Test Remain Acc):     {result['TRA']:.3f}\")\n",
    "    print(f\"  FQS (Forgetting Quality):  {result['FQS']}\")\n",
    "    print(f\"  Runtime: {result['RTE']:.1f}s\")\n",
    "    print(f\"{'-'*40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: Gradient Ascent Unlearning Results\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<15} {'UA':>8} {'RA':>8} {'TUA':>8} {'TRA':>8} {'FQS':>8} {'Time':>8}\")\n",
    "print(\"-\"*70)\n",
    "for r in results:\n",
    "    print(f\"{r['Model']:<15} {r['UA']:>8.3f} {r['RA']:>8.3f} {r['TUA']:>8.3f} {r['TRA']:>8.3f} {r['FQS']:>8.4f} {r['RTE']:>7.1f}s\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Gradient Ascent\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Trains ONLY on forget data (not retain data)\n",
    "- Uses negative loss: `loss = -criterion(outputs, labels)`\n",
    "- Gradient clipping prevents exploding gradients\n",
    "- Fast training (only processes forget samples)\n",
    "\n",
    "**Expected Behavior:**\n",
    "- UA (Unlearning Accuracy) should decrease towards random chance (~10%)\n",
    "- RA (Remain Accuracy) may decrease due to catastrophic forgetting\n",
    "- Trade-off: Strong forgetting vs. preserving performance on other classes\n",
    "\n",
    "**Hyperparameter Sensitivity:**\n",
    "- Learning rate: Higher LR = faster/stronger forgetting but more damage to other classes\n",
    "- Epochs: More epochs = more forgetting but risk of over-forgetting\n",
    "- Gradient clipping: Prevents instability during gradient ascent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
